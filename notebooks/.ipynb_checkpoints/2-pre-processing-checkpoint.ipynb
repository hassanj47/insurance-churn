{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "155e05be-2f7a-49f5-a278-a096726ad1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ac2db75-8500-4034-98bb-0b06b81e571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36eb7fdf-d158-41c6-b0cf-81516c6f74c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if not exists\n",
    "output_dir = \"../data/outputs/\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fe1700c-df1c-4b5c-a6f8-89ec7e1ae087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "# file_path = \"data/inputs/insurance_data.csv\"\n",
    "# df = pd.read_csv(file_path)\n",
    "# Define S3 bucket and file details\n",
    "bucket_name = \"refocus-storage\"\n",
    "filename = 'insurance_data_2025-02-11_02-45-54'\n",
    "file_key = \"inusrance-data-raw/{}.csv\".format(filename)\n",
    "\n",
    "# Create S3 client\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Read CSV file from S3\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "csv_data = obj[\"Body\"].read().decode(\"utf-8\")\n",
    "\n",
    "# Convert CSV data to pandas DataFrame\n",
    "df = pd.read_csv(StringIO(csv_data))\n",
    "\n",
    "print(\"Dataset Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6712c93c-d6d3-4c11-8859-437a8e3c2e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Handled.\n"
     ]
    }
   ],
   "source": [
    "# ---- STEP 1: HANDLE MISSING VALUES ----\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# If missing values exist, impute with mean/median/mode as appropriate\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)  # Fill categorical with mode\n",
    "        else:\n",
    "            df[col].fillna(df[col].median(), inplace=True)  # Fill numerical with median\n",
    "\n",
    "print(\"Missing Values Handled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4966d98b-1b19-4a0c-9588-ed2ad7967ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates Removed.\n"
     ]
    }
   ],
   "source": [
    "# ---- STEP 2: REMOVE DUPLICATES ----\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"Duplicates Removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcf41f3b-936b-4108-9469-60428798766d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Values Handled.\n"
     ]
    }
   ],
   "source": [
    "# ---- STEP 3: HANDLE NEGATIVE VALUES ----\n",
    "# Check and correct any negative values in numerical columns\n",
    "num_columns = [\"age\", \"annual_premium\", \"claims_count\"]\n",
    "for col in num_columns:\n",
    "    df[col] = df[col].apply(lambda x: np.abs(x))  # Take absolute value if negative\n",
    "\n",
    "print(\"Negative Values Handled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14c0753e-e549-47ab-96f8-59ed36c67e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 4: FEATURE ENGINEERING ----\n",
    "# 4.1 Convert categorical column \"policy_type\" into dummy variables\n",
    "df = pd.get_dummies(df, columns=[\"policy_type\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa362dd2-df7d-44d5-ae1f-7ff038a59b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Normalize numerical features using Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "df[num_columns] = scaler.fit_transform(df[num_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76f9ced3-87ba-4f9c-9e28-bb6c8b615a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering Completed.\n"
     ]
    }
   ],
   "source": [
    "# 4.3 Ensure target variable is properly formatted\n",
    "df[\"churn\"] = df[\"churn\"].astype(int)\n",
    "\n",
    "print(\"Feature Engineering Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27fbc934-6391-4e8a-9803-d824b75db25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Data Saved at: data/outputs/cleaned_insurance_data.csv\n",
      "Feature-Engineered Data Saved at: data/outputs/features_insurance_data.csv\n"
     ]
    }
   ],
   "source": [
    "# ---- STEP 5: SAVE CLEANED AND FEATURE ENGINEERED DATA ----\n",
    "cleaned_data_path = os.path.join(output_dir, \"cleaned_insurance_data.csv\")\n",
    "features_data_path = os.path.join(output_dir, \"features_insurance_data.csv\")\n",
    "\n",
    "# Save cleaned dataset before feature engineering\n",
    "df.to_csv(cleaned_data_path, index=False)\n",
    "print(f\"Cleaned Data Saved at: {cleaned_data_path}\")\n",
    "\n",
    "# Save fully processed dataset with feature engineering\n",
    "df.to_csv(features_data_path, index=False)\n",
    "print(f\"Feature-Engineered Data Saved at: {features_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b4f3662-3674-4b4e-98ab-30827a7a53cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File successfully uploaded to s3://refocus-storage/insurance-data-processed/insurance_data_2025-02-11_02-45-54_processed.csv\n"
     ]
    }
   ],
   "source": [
    "# Define S3 bucket and file details\n",
    "bucket_name = \"refocus-storage\"\n",
    "file_key = \"insurance-data-processed/{}_processed.csv\".format(filename)\n",
    "\n",
    "# Convert DataFrame to CSV format in memory\n",
    "csv_buffer = StringIO()\n",
    "df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "# Create S3 client\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Upload CSV file to S3\n",
    "s3.put_object(Bucket=bucket_name, Key=file_key, Body=csv_buffer.getvalue())\n",
    "\n",
    "print(f\"File successfully uploaded to s3://{bucket_name}/{file_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cd1f3c5-af0c-478d-989a-d17a79f94714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Preprocessing Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nData Preprocessing Completed Successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
