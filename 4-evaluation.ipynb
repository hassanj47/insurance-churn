{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "395019e2-e68e-41a6-ab08-071aa068ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8980f6f4-e9df-4609-b680-e39d15ce5e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set directories\n",
    "output_dir = \"data/outputs/\"\n",
    "report_dir = \"reports/\"\n",
    "\n",
    "# Create reports directory if it doesn't exist\n",
    "os.makedirs(report_dir, exist_ok=True)\n",
    "\n",
    "# Define report file paths\n",
    "evaluation_file = os.path.join(report_dir, \"evaluation_summary.csv\")\n",
    "benchmark_file = os.path.join(report_dir, \"benchmarks.csv\")\n",
    "\n",
    "# Get current date\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Define model prediction files\n",
    "model_predictions = {\n",
    "    \"Logistic Regression\": os.path.join(output_dir, \"lr_model_predictions.csv\"),\n",
    "    \"Random Forest\": os.path.join(output_dir, \"rf_model_predictions.csv\"),\n",
    "    \"XGBoost\": os.path.join(output_dir, \"xgboost_model_predictions.csv\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9530dae4-1859-4494-9f82-1362ff85704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store results\n",
    "evaluation_results = []\n",
    "\n",
    "# ---- STEP 1: CALCULATE METRICS FOR EACH MODEL ----\n",
    "for model_name, file_path in model_predictions.items():\n",
    "    if os.path.exists(file_path):  # Check if predictions file exists\n",
    "        # Read predictions\n",
    "        df = pd.read_csv(file_path)\n",
    "        y_true = df[\"Actual\"]\n",
    "        y_pred = df[\"Predicted\"]\n",
    "        y_pred_proba = df[\"Predicted_Probability\"]\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics = {\n",
    "            \"Model\": model_name,\n",
    "            \"Date\": current_date,\n",
    "            \"Data\": os.path.basename(file_path),\n",
    "            \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"Precision\": precision_score(y_true, y_pred),\n",
    "            \"Recall\": recall_score(y_true, y_pred),\n",
    "            \"F1 Score\": f1_score(y_true, y_pred),\n",
    "            \"AUC-ROC\": roc_auc_score(y_true, y_pred_proba),\n",
    "        }\n",
    "\n",
    "        # Append to results list\n",
    "        evaluation_results.append(metrics)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "evaluation_df = pd.DataFrame(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd456e-dac5-43d8-bda3-c1105c2031b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 2: SAVE EVALUATION SUMMARY ----\n",
    "evaluation_df.to_csv(evaluation_file, index=False)\n",
    "print(f\"Evaluation Summary Saved at: {evaluation_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf9da98-5d66-4c46-ab1a-5a937bb7b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 3: IDENTIFY BEST MODEL BASED ON F1-SCORE ----\n",
    "best_model = evaluation_df.sort_values(by=\"F1 Score\", ascending=False).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fd8d59-409e-4afd-91d7-cc256e102cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 4: UPDATE BENCHMARKS ----\n",
    "if os.path.exists(benchmark_file):\n",
    "    benchmarks_df = pd.read_csv(benchmark_file)\n",
    "    benchmarks_df = pd.concat([benchmarks_df, best_model.to_frame().T], ignore_index=True)\n",
    "else:\n",
    "    benchmarks_df = pd.DataFrame([best_model])\n",
    "\n",
    "# Save benchmark file\n",
    "benchmarks_df.to_csv(benchmark_file, index=False)\n",
    "print(f\"Benchmark Updated at: {benchmark_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ecd7a2-6e26-4b80-b2f8-c35b07a2ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 5: VISUALIZE MODEL PERFORMANCE ----\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Plot comparison of metrics (without highlighting best model)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "evaluation_df.set_index(\"Model\")[[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"AUC-ROC\"]].plot(kind=\"bar\", ax=ax, colormap=\"coolwarm\")\n",
    "\n",
    "ax.set_title(\"Model Performance Metrics\", fontsize=14)\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend(loc=\"lower right\")\n",
    "\n",
    "# Show plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1779146-f51b-4071-b200-820d0d9c560d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
