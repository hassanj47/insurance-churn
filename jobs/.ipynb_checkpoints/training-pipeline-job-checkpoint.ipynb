{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a3550-bb3d-4a48-957e-c0e98dedadc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "!pip install imblearn\n",
    "import os\n",
    "import boto3\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from io import StringIO\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# ---- AWS CONFIGURATIONS ----\n",
    "S3_BUCKET = \"refocus-storage\"\n",
    "RAW_DATA_PREFIX = \"insurance-data-raw/\"\n",
    "PROCESSED_DATA_PREFIX = \"insurance-data-processed/\"\n",
    "MODEL_REGISTRY_PATH = \"model-registry/\"\n",
    "BENCHMARKS_PATH = \"benchmarks/benchmarks.csv\"\n",
    "BEST_MODEL_PATH = f\"{MODEL_REGISTRY_PATH}best_model.pkl\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def get_latest_file_from_s3(prefix):\n",
    "    \"\"\"Retrieve the latest file based on timestamp in the filename from an S3 folder.\"\"\"\n",
    "    response = s3.list_objects_v2(Bucket=S3_BUCKET, Prefix=prefix)\n",
    "\n",
    "    # Extract only valid filenames that match the expected pattern dynamically using prefix\n",
    "    pattern = rf\"{re.escape(prefix)}insurance_data_\\d{{4}}-\\d{{2}}-\\d{{2}}_\\d{{2}}-\\d{{2}}-\\d{{2}}\\.csv\"\n",
    "    \n",
    "    files = [obj['Key'] for obj in response.get('Contents', []) if re.search(pattern, obj['Key'])]\n",
    "\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No valid timestamped files found in S3 under {prefix}\")\n",
    "\n",
    "    # Sort files by timestamp (most recent first)\n",
    "    files.sort(reverse=True, key=lambda x: re.search(r\"(\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2})\", x).group(1))\n",
    "\n",
    "    # Get the latest file and extract its timestamp\n",
    "    latest_file = files[0]\n",
    "    timestamp_match = re.search(r\"(\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2})\", latest_file)\n",
    "    \n",
    "    if not timestamp_match:\n",
    "        raise ValueError(f\"Timestamp extraction failed for {latest_file}\")\n",
    "\n",
    "    return latest_file, timestamp_match.group(1)\n",
    "\n",
    "\n",
    "# ---- STEP 1: LOAD & PREPROCESS DATA ----\n",
    "latest_file, timestamp = get_latest_file_from_s3(RAW_DATA_PREFIX)\n",
    "print(f\"Loading latest data file: {latest_file}\")\n",
    "\n",
    "obj = s3.get_object(Bucket=S3_BUCKET, Key=latest_file)\n",
    "df = pd.read_csv(StringIO(obj[\"Body\"].read().decode(\"utf-8\")))\n",
    "\n",
    "# Handle missing values\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "        else:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "# Remove duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Convert categorical features\n",
    "df = pd.get_dummies(df, columns=[\"policy_type\"], drop_first=True)\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = MinMaxScaler()\n",
    "df[[\"age\", \"annual_premium\", \"claims_count\"]] = scaler.fit_transform(df[[\"age\", \"annual_premium\", \"claims_count\"]])\n",
    "\n",
    "# Save processed data to S3 with the same timestamp\n",
    "processed_data_path = f\"{PROCESSED_DATA_PREFIX}insurance_data_{timestamp}_processed.csv\"\n",
    "csv_buffer = StringIO()\n",
    "df.to_csv(csv_buffer, index=False)\n",
    "s3.put_object(Bucket=S3_BUCKET, Key=processed_data_path, Body=csv_buffer.getvalue())\n",
    "\n",
    "print(\"Data preprocessing complete!\")\n",
    "\n",
    "# ---- STEP 2: TRAINING MODELS ----\n",
    "X = df.drop(columns=[\"churn\", \"customer_id\"])\n",
    "y = df[\"churn\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define hyperparameter grids\n",
    "param_grids = {\n",
    "    \"logistic_regression\": {\n",
    "        \"C\": [0.01, 0.1, 1, 10, 100],\n",
    "        \"solver\": [\"liblinear\", \"lbfgs\"]\n",
    "    },\n",
    "    \"random_forest\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [10, 20, None],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4]\n",
    "    },\n",
    "    \"xgboost\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "        \"max_depth\": [3, 6, 9],\n",
    "        \"subsample\": [0.8, 1.0],\n",
    "        \"colsample_bytree\": [0.8, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"Logistic_Regression\": LogisticRegression(max_iter=500, random_state=42),\n",
    "    \"Random_Forest\": RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric=\"logloss\", random_state=42)\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "model_performance = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    # model.fit(X_train_resampled, y_train_resampled)\n",
    "    grid_search = GridSearchCV(model, param_grids[model_name.lower().replace(\" \", \"_\")], cv=StratifiedKFold(n_splits=5), scoring=\"f1\", n_jobs=-1)\n",
    "    grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_models[model_name] = best_model\n",
    "    \n",
    "    \n",
    "    # y_pred = model.predict(X_test)\n",
    "    # y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        # Make predictions\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "   \n",
    "    metrics = {\n",
    "        \"Timestamp\": timestamp,\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1 Score\": f1_score(y_test, y_pred),\n",
    "        \"AUC-ROC\": roc_auc_score(y_test, y_pred_proba),\n",
    "    }\n",
    "\n",
    "    model_performance.append(metrics)\n",
    "\n",
    "performance_df = pd.DataFrame(model_performance)\n",
    "\n",
    "# ---- STEP 3: COMPARE WITH EXISTING BEST MODEL ----\n",
    "try:\n",
    "    obj = s3.get_object(Bucket=S3_BUCKET, Key=BENCHMARKS_PATH)\n",
    "    current_benchmarks_df = pd.read_csv(StringIO(obj[\"Body\"].read().decode(\"utf-8\")))\n",
    "except:\n",
    "    current_benchmarks_df = pd.DataFrame()\n",
    "\n",
    "updated_benchmarks_df = pd.concat([current_benchmarks_df, performance_df], ignore_index=True)\n",
    "\n",
    "benchmarks_buffer = StringIO()\n",
    "updated_benchmarks_df.to_csv(benchmarks_buffer, index=False)\n",
    "s3.put_object(Bucket=S3_BUCKET, Key=BENCHMARKS_PATH, Body=benchmarks_buffer.getvalue())\n",
    "\n",
    "best_new_model = performance_df.sort_values(by=\"F1 Score\", ascending=False).iloc[0]\n",
    "\n",
    "if not current_benchmarks_df.empty:\n",
    "    latest_benchmark = current_benchmarks_df.sort_values(by=\"Timestamp\", ascending=False).iloc[0]\n",
    "else:\n",
    "    latest_benchmark = None\n",
    "\n",
    "if latest_benchmark is None or best_new_model[\"F1 Score\"] > latest_benchmark[\"F1 Score\"]:\n",
    "    best_model_name = best_new_model[\"Model\"]\n",
    "    #best_model_file = f\"{MODEL_REGISTRY_PATH}insurance_data_{timestamp}_{best_model_name.lower().replace(' ', '_')}.pkl\"\n",
    "    best_model_file = f\"{MODEL_REGISTRY_PATH}best_model.pkl\"\n",
    "\n",
    "    #best_model_file = 'best_model.pkl'\n",
    "    best_model = best_models[best_model_name]  # Get the trained model\n",
    "    model_buffer = pickle.dumps(best_model)\n",
    "    s3.put_object(Bucket=S3_BUCKET, Key=best_model_file, Body=model_buffer)\n",
    "\n",
    "    print(f\"New best model saved: {best_model_file}\")\n",
    "\n",
    "else:\n",
    "    best_model_name = best_new_model[\"Model\"]\n",
    "    best_model_file = f\"{MODEL_REGISTRY_PATH}{timestamp}_{best_model_name.lower().replace(' ', '_')}.pkl\"\n",
    "    best_model = best_models[best_model_name]  # Get the trained model\n",
    "    model_buffer = pickle.dumps(best_model)\n",
    "    s3.put_object(Bucket=S3_BUCKET, Key=best_model_file, Body=model_buffer)\n",
    "    \n",
    "    print(\"No model update needed. Current best model remains.\")\n",
    "\n",
    "print(\"Training, evaluation, and deployment process completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
